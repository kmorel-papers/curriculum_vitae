@Article{Athawale2025,
  author   = {Athawale, Tushar M. and Wang, Zhe and Pugmire, David and Moreland, Kenneth and Gong, Qian and Klasky, Scott and Johnson, Chris R. and Rosen, Paul},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  title    = {Uncertainty Visualization of Critical Points of {2D} Scalar Fields for Parametric and Nonparametric Probabilistic Models},
  year     = {2025},
  month    = jan,
  number   = {1},
  pages    = {108--118},
  volume   = {31},
  abstract = {This paper presents a novel end-to-end framework for closed-form computation and visualization of critical point uncertainty in 2D uncertain scalar fields. Critical points are fundamental topological descriptors used in the visualization and analysis of scalar fields. The uncertainty inherent in data (e.g., observational and experimental data, approximations in simulations, and compression), however, creates uncertainty regarding critical point positions. Uncertainty in critical point positions, therefore, cannot be ignored, given their impact on downstream data analysis tasks. In this work, we study uncertainty in critical points as a function of uncertainty in data modeled with probability distributions. Although Monte Carlo (MC) sampling techniques have been used in prior studies to quantify critical point uncertainty, they are often expensive and are infrequently used in production-quality visualization software. We, therefore, propose a new end-to-end framework to address these challenges that comprises a threefold contribution. First, we derive the critical point uncertainty in closed form, which is more accurate and efficient than the conventional MC sampling methods. Specifically, we provide the closed-form and semianalytical (a mix of closed-form and MC methods) solutions for parametric (e.g., uniform, Epanechnikov) and nonparametric models (e.g., histograms) with finite support. Second, we accelerate critical point probability computations using a parallel implementation with the VTK-m library, which is platform portable. Finally, we demonstrate the integration of our implementation with the ParaView software system to demonstrate near-real-time results for real datasets.},
  comment  = {An algorithm to determine the probable location of critical points in a field with uncertainty. This is done by looking at the possible values at each point and determining the probabity of any condition that leads to a critical point in the cell. A limitation of this work is that it only works on independent values.},
  doi      = {10.1109/TVCG.2024.3456393},
}

@Article{Moreland2024:IJHPCA,
  author  = {Kenneth Moreland and Tushar M. Athawale and Vicente Bolea and Mark Bolstad and Eric Brugger and Hank Childs and Axel Huebl and Li-Ta Lo and Berk Geveci and Nicole Marsaglia and Sujin Philip and David Pugmire and Silvio Rizzi and Zhe Wang and Abhishek Yenpure},
  journal = {The International Journal of High Performance Computing Applications},
  title   = {{Visualization at exascale: Making it all work with VTK-m}},
  year    = {2024},
  month   = aug,
  number  = {5},
  pages   = {508--526},
  volume  = {38},
  doi     = {10.1177/10943420241270969},
}

@Article{Samsel2024,
  author  = {Francesca Samsel and W. Alan Scott and Kenneth Moreland},
  journal = {IEEE Computer Graphics and Applications},
  title   = {A New Default Colormap for {ParaView}},
  year    = {2024},
  month   = jul,
  number  = {4},
  pages   = {150--160},
  volume  = {44},
  doi     = {10.1109/MCG.2024.3383137},
}

@Article{Moreland2021,
  author   = {Kenneth Moreland and Robert Maynard and David Pugmire and Abhishek Yenpure and Allison Vacanti and Matthew Larsen and Hank Childs},
  journal  = {Parallel Computing},
  title    = {Minimizing Development Costs for Efficient Many-Core Visualization Using {MCD$^3$}},
  year     = {2021},
  month    = dec,
  number   = {102834},
  volume   = {108},
  abstract = {Scientific visualization software increasingly needs to support many-core architectures. However, development time is a significant challenge due to the breadth and diversity of both visualization algorithms and architectures. With this work, we introduce a development environment for visualization algorithms on many-core devices that extends the traditional data-parallel primitive (DPP) approach with several existing constructs and an important new construct: meta-DPPs. We refer to our approach as MCD3 --- Meta-DPPs, Convenience routines, Data management, DPPs, and Devices. The twin goals of MCD3 are to reduce developer time and to deliver efficient performance on many-core architectures, and our evaluation considers both of these goals. For development time, we study 57 algorithms implemented in the VTK-m software library and determine that MCD3 leads to significant savings. For efficient performance, we survey ten studies looking at individual algorithms and determine that the MCD3 hardware-agnostic approach leads to performance comparable to hardware-specific approaches: sometimes better, sometimes worse, and better in the aggregate. In total, we find that MCD3 is an effective approach for scientific visualization libraries to support many-core architectures.},
  comment  = {Paper that describes how VTK-m simplifies development of visualization algorithms. It did a code analysis to see how much code was actually using the abstract structures made. It also provided a small literature review of papers that compared VTK-m to equivalent algorithms developed for specific hardware and showed that VTK-m generally performed as well (sometimes better).},
  doi      = {10.1016/j.parco.2021.102834},
}

@Article{Moreland2020:SIAM,
  author  = {Kenneth Moreland and Hank Childs},
  journal = {SIAM News},
  title   = {Scientific Visualization: New Techniques in Production Software},
  year    = {2020},
  month   = nov,
  url     = {https://sinews.siam.org/Details-Page/scientific-visualization-new-techniques-in-production-software},
}

@Article{Childs2020,
  author   = {Hank Childs and others},
  journal  = {The International Journal of High Performance Computing Applications},
  title    = {A terminology for in situ visualization and analysis systems},
  year     = {2020},
  month    = aug,
  abstract = {The term "in situ processing" has evolved over the last decade to mean both a specific strategy for visualizing and analyzing data and an umbrella term for a processing paradigm. The resulting confusion makes it difficult for visualization and analysis scientists to communicate with each other and with their stakeholders. To address this problem, a group of over 50 experts convened with the goal of standardizing terminology. This paper summarizes their findings and proposes a new terminology for describing in situ systems. An important finding from this group was that in situ systems are best described via multiple, distinct axes: integration type, proximity, access, division of execution, operation controls, and output type. This paper discusses these axes, evaluates existing systems within the axes, and explores how currently used terms relate to the axes.},
  comment  = {The final publication of the in situ terminology project. This project provides a taxonomy of in situ visualization types along 6 axes: integration type, proximity, access, division of execution, operation controls, and output type.},
  doi      = {10.1177/1094342020935991},
}

@InProceedings{Choi2018,
  author    = {J. Y. Choi and C. Chang and J. Dominski and S. Klasky and G. Merlo and E. Suchyta and M. Ainsworth and B. Allen and F. Cappello and M. Churchill and P. Davis and S. Di and G. Eisenhauer and S. Ethier and I. Foster and B. Geveci and H. Guo and K. Huck and F. Jenko and M. Kim and J. Kress and S. Ku and Q. Liu and J. Logan and A. Malony and K. Mehta and K. Moreland and T. Munson and M. Parashar and T. Peterka and N. Podhorszki and D. Pugmire and O. Tugluk and R. Wang and B. Whitney and M. Wolf and C. Wood},
  booktitle = {2018 IEEE 14th International Conference on e-Science (e-Science)},
  title     = {Coupling Exascale Multiphysics Applications: Methods and Lessons Learned},
  year      = {2018},
  month     = oct,
  pages     = {442--452},
  abstract  = {With the growing computational complexity of science and the complexity of new and emerging hardware, it is time to re-evaluate the traditional monolithic design of computational codes. One new paradigm is constructing larger scientific computational experiments from the coupling of multiple individual scientific applications, each targeting their own physics, characteristic lengths, and/or scales. We present a framework constructed by leveraging capabilities such as in-memory communications, workflow scheduling on HPC resources, and continuous performance monitoring. This code coupling capability is demonstrated by a fusion science scenario, where differences between the plasma at the edges and at the core of a device have different physical descriptions. This infrastructure not only enables the coupling of the physics components, but it also connects in situ or online analysis, compression, and visualization that accelerate the time between a run and the analysis of the science content. Results from runs on Titan and Cori are presented as a demonstration.},
  comment   = {A paper documenting the code coupling layer of WDMApp (mostly using ADIOS). VTK-m is part of the coupling that is documented in the paper.},
  doi       = {10.1109/eScience.2018.00133},
}

@Article{Deelman2018,
  author   = {Ewa Deelman and Tom Peterka and Ilkay Altintas and Christopher D Carothers and Kerstin Kleese van Dam and Kenneth Moreland and Manish Parashar and Lavanya Ramakrishnan and Michela Taufer and Jeffrey Vetter},
  title    = {The future of scientific workflows},
  journal  = {International Journal of High Performance Computing Applications},
  year     = {2018},
  volume   = {32},
  number   = {1},
  pages    = {159--175},
  month    = jan,
  abstract = {Today’s computational, experimental, and observational sciences rely on computations that involve many related tasks. The success of a scientific mission often hinges on the computer automation of these workflows. In April 2015, the US Department of Energy (DOE) invited a diverse group of domain and computer scientists from national laboratories supported by the Office of Science, the National Nuclear Security Administration, from industry, and from academia to review the workflow requirements of DOE’s science and national security missions, to assess the current state of the art in science workflows, to understand the impact of emerging extreme-scale computing systems on those workflows, and to develop requirements for automated workflow management in future and existing environments. This article is a summary of the opinions of over 50 leading researchers attending this workshop. We highlight use cases, computing systems, workflow needs and conclude by summarizing the remaining challenges this community sees that inhibit large-scale scientific workflows from becoming a mainstream tool for extreme-scale science},
  comment  = {A paper providing the main details of the future of scientific workflows workshop. The full report is at [Workflows2015].},
  doi      = {10.1177/1094342017704893},
}

@Article{Bauer2016,
  author   = {Andrew C. Bauer and Hasan Abbasi and James Ahrens and Hank Childs and Berk Geveci and Scott Klasky and Kenneth Moreland and Patrick O'Leary and Venkatram Vishwanath and Brad Whitlock and E. Wes Bethel},
  title    = {In Situ Methods, Infrastructures, and Applications on High Performance Computing Platforms},
  journal  = {Computer Graphics Forum},
  year     = {2016},
  volume   = {35},
  number   = {3},
  pages    = {577--597},
  month    = {June},
  abstract = {The considerable interest in the high performance computing (HPC) community regarding analyzing and visualization data without first writing to disk, i.e., in situ processing, is due to several factors. First is an I/O cost savings, where data is analyzed /visualized while being generated, without first storing to a filesystem. Second is the potential for increased accuracy, where fine temporal sampling of transient analysis might expose some complex behavior missed in coarse temporal sampling. Third is the ability to use all available resources, CPU's and accelerators, in the computation of analysis products. This STAR paper brings together researchers, developers and practitioners using in situ methods in extreme-scale HPC with the goal to present existing methods, infrastructures, and a range of computational science and engineering applications using in situ analysis and visualization.},
  comment  = {A State of the Art report on production in situ technologies. ParaView Catalyst and VisIt Libsim are featured as well as ADIOS and GLEAN. Multiple examples of in situ visualization are given. There are also brief descriptions of a list of existing products.},
  doi      = {10.1111/cgf.12930},
  url      = {https://diglib.eg.org/handle/10.1111/cgf12930},
}

@Article{Moreland2016:VTKm,
  author   = {Kenneth Moreland and Christopher Sewell and William Usher and Li-Ta Lo and Jeremy Meredith and David Pugmire and James Kress and Hendrik Schroots and Kwan-Liu Ma and Hank Childs and Matthew Larsen and Chun-Ming Chen and Robert Maynard and Berk Geveci},
  title    = {{VTK-m}: Accelerating the Visualization Toolkit for Massively Threaded Architectures},
  journal  = {IEEE Computer Graphics and Applications},
  year     = {2016},
  volume   = {36},
  number   = {3},
  pages    = {48--58},
  month    = {May/June},
  abstract = {One of the most critical challenges for high-performance computing (HPC) scientific visualization is execution on massively threaded processors. Of the many fundamental changes we are seeing in HPC systems, one of the most profound is a reliance on new processor types optimized for execution bandwidth over latency hiding. Our current production scientific visualization software is not designed for these new types of architectures. To address this issue, the VTK-m framework serves as a container for algorithms, provides flexible data representation, and simplifies the design of visualization algorithms on new and future computer architecture.},
  comment  = {The first comprehensive publication for VTK-m.},
  doi      = {10.1109/MCG.2016.48},
  url      = {http://dx.doi.org/10.1109/MCG.2016.48},
}

@Article{Moreland2016:VisView,
  author   = {Kenneth Moreland},
  title    = {The Tensions of In Situ Visualization},
  journal  = {IEEE Computer Graphics and Applications},
  year     = {2016},
  volume   = {36},
  number   = {2},
  pages    = {5--9},
  month    = {March/April},
  abstract = {In situ visualization is the coupling of visualization software with a simulation or other data producer to process the data "in memory" before the data are offloaded to a storage system. Although in situ visualization provides superior analysis, it has implementation tradeoffs resulting from conflicts with some traditional expected requirements. Numerous conflicting requirements create tensions that lead to difficult implementation tradeoffs. This article takes a look at the most prevailing tensions of in situ visualization.},
  comment  = {A visualization viewpoints article about the often conflicting requirements of in situ visualization and how compromises are made in practice.},
  doi      = {10.1109/MCG.2016.35},
  url      = {http://dx.doi.org/10.1109/MCG.2016.35},
}

@InProceedings{Moreland2015:ISC,
  author    = {Kenneth Moreland and Ron Oldfield},
  booktitle = {ISC High Performance},
  title     = {Formal Metrics for Large-Scale Parallel Performance},
  year      = {2015},
  month     = jun,
  pages     = {488--496},
  abstract  = {Performance measurement of parallel algorithms is well studied and well understood. However, a flaw in traditional performance metrics is that they rely on comparisons to serial performance with the same input. This comparison is convenient for theoretical complexity analysis but impossible to perform in large-scale empirical studies with data sizes far too large to run on a single serial computer. Consequently, scaling studies currently rely on ad hoc methods that, although effective, have no grounded mathematical models. In this position paper we advocate using a rate-based model that has a concrete meaning relative to speedup and efficiency and that can be used to unify strong and weak scaling studies.},
  comment   = {Argues against using time to display the scaling behavior of large scale systems (particular for strong scaling). Instead argues the use of rate (units computed per second) as a universably comparable number. Also offers efficiency as a possible solution.},
  doi       = {10.1007/978-3-319-20119-1\_34},
  homeurl   = {/parallel-scaling-metrics/},
}

@InProceedings{Oldfield2014,
  author    = {Ron A. Oldfield and Kenneth Moreland and Nathan Fabian and David Rogers},
  title     = {Evaluation of Methods to Integrate Analysis into a Large-Scale Shock Physics Code},
  booktitle = {Proceedings of the 28th ACM international Conference on Supercomputing (ICS '14)},
  year      = {2014},
  pages     = {83--92},
  month     = {June},
  abstract  = {Exascale supercomputing will embody many revolutionary changes in the hardware and software of high-performance computing. For example, projected limitations in power and I/O-system performance will fundamentally change visualization and analysis workflows. A traditional post-processing workflow involves storing simulation results to disk and later retrieving them for visualization and data analysis; however, at Exascale, post-processing approaches will not be able to capture the volume or granularity of data necessary for analysis of these extreme-scale simulations. As an alternative, researchers are exploring ways to integrate analysis and simulation without using the storage system. In situ and in transit are two options, but there has not been an adequate evaluation of these approaches to identify strengths, weaknesses, and trade-offs at large scale. This paper provides a detailed performance and scaling analysis of a large-scale shock physics code using traditional post-processsing, in situ, and in transit analysis to detect material fragments from a simulated explosion.},
  comment   = {Follow up paper for the studies given in the technical report of [Rogers2013]. A publication of our work for the ASC II milestone for large scale in situ processing with CTH.},
  doi       = {10.1145/2597652.2597668},
}

@InProceedings{Tchoua2013,
  author    = {Roselyne Tchoua and Jong Choi and Scott Klasky and Qing Liu and Jeremy Logan and Kenneth Moreland and Jingqing Mu and Manish Parashar and Norbert Podhorszki and David Pugmire and Matthew Wolf},
  booktitle = {IEEE International Conference on eScience},
  title     = {{ADIOS} Visualization Schema: A First Step Towards Improving Interdisciplinary Collaboration in High Performance Computing},
  year      = {2013},
  month     = {October},
  pages     = {27--34},
  abstract  = {Scientific communities have benefitted from a significant increase of available computing and storage resources in the last few decades. For science projects that have access to leadership scale computing resources, the capacity to produce data has been growing exponentially. Teams working on such projects must now include, in addition to the traditional application scientists, experts in various disciplines including applied mathematicians for development of algorithms, visualization specialists for large data, and I/O specialists. Sharing of knowledge and data is becoming a requirement for scientific discovery, providing useful mechanisms to facilitate this sharing is a key challenge for e-Science. Our hypothesis is that in order to decrease the time to solution for application scientists we need to lower the barrier of entry into related computing fields. We aim at improving users' experience when interacting with a vast software ecosystem and/or huge amount of data, while maintaining focus on their primary research field. In this context we present our approach to bridge the gap between the application scientists and the visualization experts through a visualization schema as a first step and proof of concept for a new way to look at interdisciplinary collaboration among scientists dealing with big data. The key to our approach is recognizing that our users are scientists who mostly work as islands. They tend to work in very specialized environment but occasionally have to collaborate with other researchers in order to take full advantage of computing innovations and get insight from big data. We present an example of identifying the connecting elements between one of such relationships and offer a liaison schema to facilitate their collaboration.},
  comment   = {Documents the early version of the ADIOS Visualization Schema. Describes the ADIOS input XML used to define the schema as well as the attribute strings written to the ADIOS file. (This is not a complete documentation, though.) I believe the functions have been added to the ADIOS to read/define the schema since this publication.},
  doi       = {10.1109/eScience.2013.24},
  url       = {http://dx.doi.org/10.1109/eScience.2013.24},
}

@Article{Childs2013,
  author   = {Hank Childs and Berk Geveci and Will Schroeder and Jeremy Meredith and Kenneth Moreland and Christopher Sewell and Torsten Kuhlen and E. Wes Bethel},
  journal  = {IEEE Computer},
  title    = {Research Challenges for Visualization Software},
  year     = {2013},
  month    = {May},
  number   = {5},
  pages    = {34--42},
  volume   = {46},
  abstract = {As the visualization research community reorients its software to address up-coming challenges, it must successfully deal with diverse processor architectures, distributed systems, various data sources, massive parallelism, multiple input and output devices, and interactivity.},
  comment  = {This article gives a high level overview of the challenges of large-scale HPC visualization in the exascale/extreme scale from a technical standpoint.  The issues raised are: Massive Parallelization, Processor Architectures and Programming Models, Application Architecture and Data Management, Data Models, Rendering, and Interaction.},
  doi      = {10.1109/MC.2013.179},
}

@Article{Moreland2013:TVCG,
  author   = {Kenneth Moreland},
  journal  = {IEEE Transactions on Visualization and Computer Graphics},
  title    = {A Survey of Visualization Pipelines},
  year     = {2013},
  month    = {March},
  number   = {3},
  pages    = {367--378},
  volume   = {19},
  abstract = {The most common abstraction used by visualization libraries and applications today is what is known as the visualization pipeline. The visualization pipeline provides a mechanism to encapsulate algorithms and then couple them together in a variety of ways. The visualization pipeline has been in existence for over twenty years, and over this time many variations and improvements have been proposed. This paper provides a literature review of the most prevalent features of visualization pipelines and some of the most recent research directions.},
  comment  = {A survey of research pertaining to visualization pipelines, the libraries that implement them, and the tools that use them.},
  doi      = {10.1109/TVCG.2012.133},
  homeurl  = {/vis-pipelines/},
}

@InProceedings{Moreland2011:SC,
  author    = {Kenneth Moreland and Wesley Kendall and Tom Peterka and Jian Huang},
  booktitle = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC '11)},
  title     = {An Image Compositing Solution at Scale},
  year      = {2011},
  month     = {November},
  comment   = {A compendium of optimizations for sort-last image-compositing parallel rendering including  Radix-k compositing order, image compression, image interlacing, non-powers-of-two binary swap (telescoping), and improved image collection.  All the techniques are collected in the IceT parallel rendering library and scalability is shown on a petascale machine.},
  doi       = {10.1145/2063384.2063417},
  homeurl   = {/scalable-rendering/#an-image-compositing-solution-at-scale},
}

@Article{Biddiscombe2007,
  author  = {John Biddiscombe and Berk Geveci and Ken Martin and Kenneth Moreland and David Thompson},
  title   = {Time Dependent Processing in a Parallel Pipeline Architecture},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  year    = {2007},
  volume  = {13},
  number  = {6},
  pages   = {1376--1383},
  month   = {November/December},
  comment = {Adds extensions to the visualization pipeline (specifically VTK) that allow filters to report and control time.},
  doi     = {10.1109/TVCG.2007.70600},
  url     = {http://dx.doi.org/10.1109/TVCG.2007.70600},
}

@Article{Wylie2001,
  author  = {Brian Wylie and Constantine Pavlakos and Vasily Lewis and Kenneth Moreland},
  title   = {Scalable Rendering on {PC} Clusters},
  journal = {IEEE Computer Graphics and Applications},
  year    = {2001},
  volume  = {21},
  number  = {4},
  pages   = {62--70},
  month   = {July/August},
  comment = {Early article demonstrating the good scaling performance of sort-last parallel rendering.  (Does not really compare to other parallel rendering methods, although those are known to scale poorly.  You can use [Mueller1995] to argue poor scaling of sort-first.)},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;year;true;month;true;author;false;}
